{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS2500 Final Project:  Airline Customer Satisfaction Prediction\n",
    "## Team 3\n",
    "\n",
    "- Evan Chu (chu.e@northeastern.edu)\n",
    "- Vivian Li (li.viv@northeastern.edu)\n",
    "- Khushi Shah (shah.khus@northeastern.edu) \n",
    "- Daniel Veretenov (veretenov.d@northeastern.edu) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "In our project, we analyzed airline customer satisfaction data from Kaggle.com, collecting 23 factors about their flight experience. The way that our model works is that it takes in all of the data and, using an algorithm (random forest classifier), it tries to predict first what are the most important factors that airlines should consider. To validate our model, we cross-validated our predictions through testing and training the data. We also tried to eliminate factors to focus on certain factors through RFE, but realized that it created a significant decline in our accuracy. Our predictions of user satisfaction do a relatively great job of matching the actual customer satisfaction ratings with an accuracy score of 96%. We suggest that the model is rather accurate, but for real-world applications, there are some underlying biases within the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethical Considerations \n",
    "All passengers have unique flying experiences with different airlines. Our program aims to help stakeholders, such as airlines and airports, understand how they can improve customer satisfaction to ensure returning customers and profitability. Additionally, our findings would provide valuable insights to other companies in the transportation and travel industries about specific enhancements that would boost their clients' experiences.\n",
    "\n",
    "This [dataset](https://www.kaggle.com/datasets/teejmahal20/airline-passenger-satisfaction), retrieved from Kaggle, is based on an airline passenger satisfaction survey. To protect participants' privacy, all volunteers are assigned a unique ID to remain anonymous. The survey collected data about individuals' ratings of specific airline services and overall satisfaction for analysis. The data is a cleaned and modified version of the [Passenger Satisfaction dataset](https://www.kaggle.com/datasets/johndddddd/customer-satisfaction).\n",
    "\n",
    "It is unclear whether there are biases in selecting participants and collecting data. We assume that all surveys were sent to all passengers from various airlines after their flights. Participation in the survey is voluntary and optional for all users. Participants have the right to skip any questions and withdraw at any time during the survey process. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "The Covid-19 pandemic has caused severe damage to the travel industry, resulting in loss of revenues and bankruptcies. According to the [World Travel and Tourism Council](https://www.wttc.com/pandemic/), the travel and tourism sector experienced a decrease of \\\\$4.5 trillion in the global GDP with a loss of 62 million jobs worldwide. In addition, the International Air Transport Association reported that airlines worldwide had lost \\\\$126.4 billion in [2020](https://www.iata.org/en/pressroom/pr/2021-02-03-02/), followed by a loss of \\\\$47.7 billion in [2021](https://www.iata.org/en/pressroom/pr/2021-05-19-02/).\n",
    "\n",
    "As more countries are easing travel restrictions and opening up to tourists after the Covid-19 pandemic, more and more people are traveling globally. Despite the increased demand for air travel, airline customer satisfaction rates are decreasing due to rising ticket prices and overcrowding at airports. In a survey conducted by [Airports Council International](https://aci.aero/news/2020/09/29/aci-world-releases-results-of-global-survey-on-state-of-the-airport-industry/), 40% of the respondents stated that their airport experience had worsened due to the pandemic, and many were frustrated with the airport procedures and wait times. Furthermore, some of the airline services have decreased. According to [CNN](https://www.cnn.com/travel/article/airline-satisfaction-plummets-covid-intl-hnk-dst/index.html), there was a “decrease in food and beverage satisfaction in premium economy and business and the fact that many airlines didn’t serve alcohol on board for much of last year”.\n",
    "\n",
    "As mentioned previously in the Ethical Considerations section, this project seeks to help the air travel industry better its services to achieve good customer satisfaction. We will cluster the passengers into sets of classes based on the class of ticket that they purchased. Doing this allows us to discover if there are any trends among their overall satisfaction. The machine learning methods we use will take different factors of satisfaction into account to give us an understanding of the importance of each variable in determining customer satisfaction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data description  \n",
    "Data set comes from Kaggle.com which consists of a csv file of customer’s satisfaction of their airline experience. There are 23 factors that led to the customer’s satisfaction.  <br> \n",
    "\n",
    "<u> **Features** </u> <br> \n",
    "**Gender:** Gender of the passengers (Female, Male)  <br> \n",
    "**Customer Type:** The customer type (Loyal customer, disloyal customer)  <br> \n",
    "**Age:** The actual age of the passengers  <br> \n",
    "**Type of Travel:** Purpose of the flight of the passengers (Personal Travel, Business Travel)  <br> \n",
    "**Class:** Travel class in the plane of the passengers (Business, Eco, Eco Plus)  <br> \n",
    "**Flight distance:** The flight distance of this journey  <br> \n",
    "**Inflight wifi service:** Satisfaction level of the inflight wifi service (0:Not Applicable;1-5)  <br> \n",
    "**Departure/Arrival time convenient:** Satisfaction level of Departure/Arrival time convenient  <br> \n",
    "**Ease of Online booking:** Satisfaction level of online booking  <br> \n",
    "**Gate location:** Satisfaction level of Gate location  <br> \n",
    "**Food and drink:** Satisfaction level of Food and drink  <br> \n",
    "**Online boarding:** Satisfaction level of online boarding  <br> \n",
    "**Seat comfort:** Satisfaction level of Seat comfort  <br> \n",
    "**Inflight entertainment:** Satisfaction level of inflight entertainment <br> \n",
    "**On-board service:** Satisfaction level of On-board service  <br> \n",
    "**Leg room service:** Satisfaction level of Leg room service  <br> \n",
    "**Baggage handling:** Satisfaction level of baggage handling  <br> \n",
    "**Check-in service:** Satisfaction level of Check-in service  <br> \n",
    "**Inflight service:** Satisfaction level of inflight service  <br> \n",
    "**Cleanliness:** Satisfaction level of Cleanliness  <br> \n",
    "**Departure Delay in Minutes:** Minutes delayed when departure  <br> \n",
    "**Arrival Delay in Minutes:** Minutes delayed when Arrival  <br> \n",
    "**Satisfaction:** Airline satisfaction level (Satisfaction, neutral or dissatisfaction)  <br> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SimpleImpute' from 'sklearn.impute' (/Users/vivian/opt/anaconda3/lib/python3.9/site-packages/sklearn/impute/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9m/1hg1hctj5s94qtlnx8n6561h0000gn/T/ipykernel_19835/1349218189.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleImpute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRFE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'SimpleImpute' from 'sklearn.impute' (/Users/vivian/opt/anaconda3/lib/python3.9/site-packages/sklearn/impute/__init__.py)"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import statsmodels.api as sm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImpute\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before incorporating any machine learning algorithms into the dataset, we must clean the data by eliminating any rows with missing information. We have also decided to drop the 'id' and 'Unnamed: 0' columns because they are redundant and unnecessary for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train, test], axis=0, sort=False)\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.drop('id', axis=1, inplace=True)\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create histogram of all variables\n",
    "df.hist(figsize=(20,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each individual chart (shown above) is a histogram of all the variables from the data. Since ‘Age’, ‘Flight Distance’, ‘Departure’, and ‘Arrival Delay’ are all numerical values, the values will be standardized by a standard scalar or MinMax Scaler. Meanwhile, all of the other variables are categorical. They are identified by survey variables ranging from 1 being a bad experience to 5 being a great experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we split the data into two groups: categorical and numerical data. Before any hardcoding was implemented, we conducted a thorough data types check. Our analysis revealed that 'Gender', 'Customer Type', 'Type of Travel', 'Class', and 'Satisfaction' are object data types. To be more specific, they are strings when referring to the printed dataset from the Data Description section. Meanwhile, all other data are integers or floats. However, only the 'Age', 'Flight Distance', 'Departure', and 'Arrival Delay' columns are numerical data. The remaining variables are categorical data from the survey variables ranging from 1 to 5, signifying 'bad experience' to 'great experience', respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prior to hardcoding\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To correct the data type, we hardcoded all object data types as 0, 1, or 2, each with its respective meaning, as shown in the code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Customer Type'] = df['Customer Type'].replace({'Loyal Customer': 1,'disloyal Customer': 0})\n",
    "df['Gender'] = df['Gender'].replace({'Male': 0, 'Female': 1})\n",
    "df['Type of Travel'] = df['Type of Travel'].replace({'Business travel': 0, 'Personal Travel': 1})\n",
    "df['Class'] = df['Class'].replace({'Business': 0, 'Eco': 1, 'Eco Plus': 2})\n",
    "df['satisfaction'] = df['satisfaction'].replace({'satisfied': 1, 'neutral or dissatisfied': 0})\n",
    "df = df[df['Customer Type'].isin([0, 1])]\n",
    "df = df[df['Gender'].isin([0, 1])]\n",
    "df = df[df['Type of Travel'].isin([0, 1])]\n",
    "df = df[df['Class'].isin([0, 1, 2])]\n",
    "df = df[df['satisfaction'].isin([0, 1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we corrected the mapping following the same binary as the object data type. Additionally, we hardcoded the categorical data types to change from integers to strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping \n",
    "ClassMap = {0:'Business', 1:'Eco', 2:'Eco Plus'}\n",
    "SatisfactionMap = {0:'neutral or dissatisfied', 1:'satisfied'}\n",
    "df['Class'] = df['Class'].map(ClassMap)\n",
    "df['satisfaction'] = df['satisfaction'].map(SatisfactionMap)\n",
    "df['Gender'] = df['Gender'].astype(str)\n",
    "df['Customer Type'] = df['Customer Type'].astype(str)\n",
    "df['Type of Travel'] = df['Type of Travel'].astype(str)\n",
    "df['Inflight wifi service'] = df['Inflight wifi service'].astype(str)\n",
    "df['Departure/Arrival time convenient'] = df['Departure/Arrival time convenient'].astype(str)\n",
    "df['Ease of Online booking'] = df['Ease of Online booking'].astype(str)\n",
    "df['Gate location'] = df['Gate location'].astype(str)\n",
    "df['Food and drink'] = df['Food and drink'].astype(str)\n",
    "df['Online boarding'] = df['Online boarding'].astype(str)\n",
    "df['Seat comfort'] = df['Seat comfort'].astype(str)\n",
    "df['Inflight entertainment'] = df['Inflight entertainment'].astype(str)\n",
    "df['On-board service'] = df['On-board service'].astype(str)\n",
    "df['Leg room service'] = df['Leg room service'].astype(str)\n",
    "df['Baggage handling'] = df['Baggage handling'].astype(str)\n",
    "df['Checkin service'] = df['Checkin service'].astype(str)\n",
    "df['Inflight service'] = df['Inflight service'].astype(str)\n",
    "df['Cleanliness'] = df['Cleanliness'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown below, the data type for each column has been corrected in order for us to implement the machine learning algorithms and interpret the data better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dummified the data for each categorical column, converting them into a set of binary variables. Since the majority of the machine learning algorithms are designed to work with numerical data, we had to dummify the data to use them as input features for predictions and other data processing models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "categorical_columns\n",
    "\n",
    "df_categorical = pd.get_dummies(df[categorical_columns], drop_first=True)\n",
    "df_categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the dataset, we count the satisfications from the passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calulate the percentage of satistfaction_satisfied = 1\n",
    "df['satisfaction'].value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the numerical columns, we utilized StandardScaler technique to standardize the scale of the data, making it easier for comparison. This transformed the data so that the variables will have a normal distribution with 0 mean and unit variance. The StandardScaler technique is an essential step in the process because it prevents biased or inaccurate results, which can improve the performance and accuracy of machine learning algorithms. In addition, it reduces the impact of outliers in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numerical = df.drop(categorical_columns, axis=1)\n",
    "df_numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_df = scaler.fit_transform(df_numerical)\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=df_numerical.columns)\n",
    "scaled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preparing the numerical and categorical datasets, we combined them into a new table that we will use for the machine learning algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_data = pd.concat([df_categorical, scaled_df], axis=1)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a pie chart to demonstrate the satisfied percentage vs unsatisfied percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a pie chart with the percentage of all_data['satisfaction_satisfied'] is 1 or 0\n",
    "# Count the number of occurrences of each value in the 'satisfaction_satisfied' column\n",
    "value_counts = all_data['satisfaction_satisfied'].value_counts()\n",
    "\n",
    "# Create a pie chart with the percentage of each value\n",
    "plt.pie(value_counts, labels=['Not Satisfied', 'Satisfied'], autopct='%1.1f%%')\n",
    "plt.title('Satisfaction Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a heatmap to see the correlation between the variables\n",
    "# plt.figure(figsize=(40,40))\n",
    "# sns.heatmap(all_data.corr(), annot=True, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a heatmap of the correlation of th varibales with the satisfaction\n",
    "plt.figure(figsize=(7,25))\n",
    "sns.heatmap(all_data.corr()[['satisfaction_satisfied']].sort_values(by='satisfaction_satisfied', ascending=False), annot=True, cmap='coolwarm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap of the correlation of all of the variables cleaned to the satisfaction_satisfied variable. It shows that a better online boarding system and good/faster inflight wifi would possibly lead to more percentage of satisfied customers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reclean the data set to ensure there are no missing values\n",
    "all_data = all_data.dropna()\n",
    "df_predictors = all_data.drop('satisfaction_satisfied', axis=1)\n",
    "target = all_data['satisfaction_satisfied']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting any modeling, we split the data into 2 parts: test and train, with a split of 20% for test data and 80% for train data, which was deemed sufficient to properly test the model given the amount of data available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_predictors, target, test_size=0.2, random_state=42)\n",
    "print('x_train', X_train.shape)\n",
    "print('x_test', X_test.shape)\n",
    "print('y_train', y_train.shape)\n",
    "print('y_test', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning algorithm that we will be using to analyze the data is Random Forest Classification with Hyperparameter tuning and F1 score. We will explain the outcomes using recursive feature elimination, confusion matrix, and feature importance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def confusion_mat(y_t, y_p):\n",
    "    cnf_matrix = confusion_matrix(y_t, y_p)\n",
    "\n",
    "    cf_df = pd.DataFrame(cnf_matrix, index = ['Actual 0', 'Actual 1'], columns = ['Predicted 0', 'Predicted 1'])\n",
    "    sns.set(font_scale=1.4)\n",
    "    plt.figure(figsize=(7,5))\n",
    "    sns.heatmap(cf_df, annot=True, fmt='g')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classification:\n",
    "We are using Random Forest Classification to predict the features that contribute to customer satisfaction and to explore the relationships between different variables in the dataset. It will also provide estimates of feature importance, which we will use to interpret the results. For the Random Forest Classification, we applied hyperparameter tuning and set several parameters before training the dataset, including n_estimators, random_state, max_depth, min_samples_split, min_samples_leaf, max_features, and bootstrap. By applying hyperparameter tuning, we can control the complexity of the model and reduce the risks of overfitting and underfitting. <br>\n",
    "\n",
    "The following explains each parameter: <br>\n",
    "\n",
    "**n_estimators:** The number of trees in the forest. In this case, it's set to 100, which means that the algorithm will create 100 decision trees. <br>\n",
    "\n",
    "**random_state:** The seed used by the random number generator. It's set to 42, which means that the algorithm will produce the same results every time it's run. <br>\n",
    "\n",
    "**max_depth:** The maximum depth of each tree. In this case, it's set to 100, which means that the algorithm will create trees with a maximum depth of 100. <br>\n",
    "\n",
    "**min_samples_split:** The minimum number of samples required to split an internal node. It's set to 1, which means that the algorithm will split nodes as long as there's at least one sample in each child node. <br>\n",
    "\n",
    "**min_samples_leaf:** The minimum number of samples required to be at a leaf node. It's set to 1, which means that the algorithm will keep splitting nodes until each leaf contains only one sample. <br>\n",
    "\n",
    "**max_features:** The number of features to consider when looking for the best split. In this case, it's set to 'auto', which means that the algorithm will consider all features for each split. <br>\n",
    "\n",
    "**bootstrap:** Whether or not to bootstrap the samples used for each tree. It's set to True, which means that the algorithm will randomly sample the data with replacement to create each tree. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest without RFE\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42, \n",
    "                                    max_depth=100, min_samples_split=1, \n",
    "                                    min_samples_leaf=1, max_features='auto', \n",
    "                                    bootstrap=True)\n",
    "model_RF = classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred_RF = model_RF.predict(X_test)\n",
    "\n",
    "# print result of FRC without RFE\n",
    "print(classification_report(y_test, y_pred_RF))\n",
    "print(confusion_mat(y_test, y_pred_RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importances = model_RF.feature_importances_\n",
    "# create a bar graph of the feature importances sorted by importance\n",
    "indices = np.argsort(importances)[::-1][:15] # select the top 20 features\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title('Feature Importances')\n",
    "plt.bar(range(15), importances[indices], color='b', align='center')\n",
    "plt.xticks(range(15), X_train.columns[indices], rotation=90)\n",
    "plt.xlim([-1, 15]) # set the x-axis limit to show only the top 20 features\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination (RFE):\n",
    "We reconducted the Random Forest Classifier with Recursive Feature Elimination for comparison and to gain a deeper understanding of the variable importance. RFE is a feature selection method that removes any weak features from the dataset. It refits the model and re-identifies the best features to focus on. RFE reduces the run time for machine learning algorithms, and it removes any irrelevant features that will decrease the model’s performance, which results in a more accurate prediction. We specified the number of features to select in our model. Thus, only the top-specified number of features will be fitted and trained in the data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the estimator (classifier) to use for feature selection\n",
    "estimator = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=100, \n",
    "                                    min_samples_split=1, min_samples_leaf=1, \n",
    "                                    max_features='auto', bootstrap=True)\n",
    "\n",
    "# create the RFE object and specify the number of features to select\n",
    "rfe = RFE(estimator, n_features_to_select=10, step=1)\n",
    "\n",
    "# fit the RFE object to the training data\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# get the top features selected by RFE\n",
    "selected_features = X_train.columns[rfe.support_]\n",
    "print('Selected features:', selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create a new DataFrame with only the selected features\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "# create a new random forest model with the selected features\n",
    "classifier_selected = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=100, \n",
    "                                              min_samples_split=1, min_samples_leaf=1, \n",
    "                                              max_features='auto', bootstrap=True)\n",
    "model_RF_selected = classifier_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred_RF_selected = model_RF_selected.predict(X_test_selected)\n",
    "\n",
    "# evaluate the performance of the model\n",
    "print(classification_report(y_test, y_pred_RF_selected))\n",
    "confusion_mat(y_test, y_pred_RF_selected)\n",
    "\n",
    "# get the feature importances\n",
    "importances_selected = model_RF_selected.feature_importances_\n",
    "\n",
    "# create a bar graph of the feature importances sorted by importance\n",
    "indices_selected = np.argsort(importances_selected)[::-1][:len(selected_features)]\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title('Feature Importances')\n",
    "plt.bar(range(len(selected_features)), importances_selected[indices_selected], color='b', align='center')\n",
    "plt.xticks(range(len(selected_features)), selected_features[indices_selected], rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result\n",
    "To interpret the results, we use classification reports to show the F1 score, confusion matrix to show the accuracy rate, and the feature importance to identify the most relevant features in the dataset. \n",
    "Print out the classification report with confusion matrix and feature importance without RFE\n",
    "\n",
    "To recall in our presentation, F1 score is a commonly used metric to evaluate the performance of a classification model. It combines the precision and recall of the model into a single score. It ranges from 0 to 1, with a higher score indicating better performance. \n",
    "\n",
    "F1 score is calculated as: F1 = 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Random Forest Classifier without RFE, we can see that the F1 score for predicting each binary is 0.96 and 0.95 for binaries 0 and 1, respectively, with an overall accuracy F1 score of 0.96. This indicates that the model has correctly identified the true positives while minimizing the false positives and false negatives. Following the confusion matrix and the report, we see the feature importance chart. As portrayed in the chart, “type of travel_1”, “online boarding_5”, “class_eco”, and “inflight_wifi_service_5” have the highest importance values. “Type of travel_1” refers to personal traveling. “Online boarding_5” and “inflight_wifi_service_5” suggest that customers are satisfied with the boarding process and the wifi services. The “class_eco” refers to the economy class seats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Random Forest Classifier with RFE, the F1 score for predicting each binary is 0.89 and 0.84 for binaries 0 and 1, respectively, with an accuracy F1 scorer of 0.87. \n",
    "These F1 scores are lower than the F1 scores from Random Forest Classifier without RFE. This change may have been caused by the accidental removal of important features while retaining the irrelevant ones. It may also be caused by eliminating too many features, resulting in an inadequate number of features to capture the underlying data accurately. Following the confusion matrix and the report, we see a slightly different feature importance chart. It shows that “flight distance”, “type of travel_1”, “online boarding_5”, “class_eco”, and “inflight wifi service_5” have the highest importance. Compared to the feature importance graph without RFE, the ranking of “flight distance” is the only difference in the chart. It was previously ranked 18th; however, it is currently the most relevant feature when predicting customer satisfaction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "Our data analysis revealed the top five improvements that airlines should focus on: online boarding experience, inflight service, class experience, inflight entertainment, and wifi experiences. These results are expected, since these five factors are all essential parts of a customer’s flight experience and are frequently advertised by quality airlines. \n",
    "\n",
    "However, these results should not be taken at face-value since biases exist in the data. For example, the dataset contains data for both loyal and disloyal customers, with loyal customers being customers who consistently travel with one airline and disloyal customers being those who travel with many different airlines. Since the data considers both types of customers, disloyal customers may have had increased dissatisfaction with an airline since their standards could have been impacted by experiences with other airlines. Another bias that exists in the data is the type of travel that customers were engaging in. Customers engaged in both business travel and in personal travel. Business travel tends to be serious and for work-related reasons, while personal travel tends to be relaxing and for enjoyment. Customers flying for personal enjoyment reasons may have been more satisfied than customers who were traveling for serious work-related reasons. Another bias that exists in the dataset is the class of ticket that the customer purchased. The dataset considers classes that include First, Business, Econ, Econ plus, etc. All of these classes provide different services at different qualities, with the highest class seats receiving better service on flights. Due to this, customers who purchased premium seats may tend to have better experiences on their flights than customers who do not. \n",
    "\n",
    "## Takeaways\n",
    "The main takeaways of this project suggest that airlines can take actions to improve customer satisfaction. These actions, mentioned above, are to improve online boarding experience, inflight service, class experience, inflight entertainment, and wifi experiences. We are confident that these actions are justified because the feature importance analysis revealed that these were the most important factors that influence customer satisfaction. Before such action is taken, some questions that might need to be answered are whether taking these actions are financially possible for the airlines, and whether the quality of the flights would increase ticket prices (which in turn may decrease customer satisfaction instead of increasing it). \n",
    "\n",
    "## Challenges\n",
    "Some challenges that were encountered during this project include learning new machine learning methods, such as F1 analysis, RFE, and etc. We learned these new methods so that we could obtain our desired results while learning and applying new methods of data analysis. Other challenges include time management with a difficult project and working with a group on a code when only one person could physically type on the Jupyter Notebook file at a time. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
